{"format": "torch", "nodes": [{"name": "Bert", "id": 139771770691728, "class_name": "DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30526, 768)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (1): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (2): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (3): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (4): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (5): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)", "parameters": [["embeddings.word_embeddings.weight", [30526, 768]], ["embeddings.position_embeddings.weight", [512, 768]], ["embeddings.LayerNorm.weight", [768]], ["embeddings.LayerNorm.bias", [768]], ["transformer.layer.0.attention.q_lin.weight", [768, 768]], ["transformer.layer.0.attention.q_lin.bias", [768]], ["transformer.layer.0.attention.k_lin.weight", [768, 768]], ["transformer.layer.0.attention.k_lin.bias", [768]], ["transformer.layer.0.attention.v_lin.weight", [768, 768]], ["transformer.layer.0.attention.v_lin.bias", [768]], ["transformer.layer.0.attention.out_lin.weight", [768, 768]], ["transformer.layer.0.attention.out_lin.bias", [768]], ["transformer.layer.0.sa_layer_norm.weight", [768]], ["transformer.layer.0.sa_layer_norm.bias", [768]], ["transformer.layer.0.ffn.lin1.weight", [3072, 768]], ["transformer.layer.0.ffn.lin1.bias", [3072]], ["transformer.layer.0.ffn.lin2.weight", [768, 3072]], ["transformer.layer.0.ffn.lin2.bias", [768]], ["transformer.layer.0.output_layer_norm.weight", [768]], ["transformer.layer.0.output_layer_norm.bias", [768]], ["transformer.layer.1.attention.q_lin.weight", [768, 768]], ["transformer.layer.1.attention.q_lin.bias", [768]], ["transformer.layer.1.attention.k_lin.weight", [768, 768]], ["transformer.layer.1.attention.k_lin.bias", [768]], ["transformer.layer.1.attention.v_lin.weight", [768, 768]], ["transformer.layer.1.attention.v_lin.bias", [768]], ["transformer.layer.1.attention.out_lin.weight", [768, 768]], ["transformer.layer.1.attention.out_lin.bias", [768]], ["transformer.layer.1.sa_layer_norm.weight", [768]], ["transformer.layer.1.sa_layer_norm.bias", [768]], ["transformer.layer.1.ffn.lin1.weight", [3072, 768]], ["transformer.layer.1.ffn.lin1.bias", [3072]], ["transformer.layer.1.ffn.lin2.weight", [768, 3072]], ["transformer.layer.1.ffn.lin2.bias", [768]], ["transformer.layer.1.output_layer_norm.weight", [768]], ["transformer.layer.1.output_layer_norm.bias", [768]], ["transformer.layer.2.attention.q_lin.weight", [768, 768]], ["transformer.layer.2.attention.q_lin.bias", [768]], ["transformer.layer.2.attention.k_lin.weight", [768, 768]], ["transformer.layer.2.attention.k_lin.bias", [768]], ["transformer.layer.2.attention.v_lin.weight", [768, 768]], ["transformer.layer.2.attention.v_lin.bias", [768]], ["transformer.layer.2.attention.out_lin.weight", [768, 768]], ["transformer.layer.2.attention.out_lin.bias", [768]], ["transformer.layer.2.sa_layer_norm.weight", [768]], ["transformer.layer.2.sa_layer_norm.bias", [768]], ["transformer.layer.2.ffn.lin1.weight", [3072, 768]], ["transformer.layer.2.ffn.lin1.bias", [3072]], ["transformer.layer.2.ffn.lin2.weight", [768, 3072]], ["transformer.layer.2.ffn.lin2.bias", [768]], ["transformer.layer.2.output_layer_norm.weight", [768]], ["transformer.layer.2.output_layer_norm.bias", [768]], ["transformer.layer.3.attention.q_lin.weight", [768, 768]], ["transformer.layer.3.attention.q_lin.bias", [768]], ["transformer.layer.3.attention.k_lin.weight", [768, 768]], ["transformer.layer.3.attention.k_lin.bias", [768]], ["transformer.layer.3.attention.v_lin.weight", [768, 768]], ["transformer.layer.3.attention.v_lin.bias", [768]], ["transformer.layer.3.attention.out_lin.weight", [768, 768]], ["transformer.layer.3.attention.out_lin.bias", [768]], ["transformer.layer.3.sa_layer_norm.weight", [768]], ["transformer.layer.3.sa_layer_norm.bias", [768]], ["transformer.layer.3.ffn.lin1.weight", [3072, 768]], ["transformer.layer.3.ffn.lin1.bias", [3072]], ["transformer.layer.3.ffn.lin2.weight", [768, 3072]], ["transformer.layer.3.ffn.lin2.bias", [768]], ["transformer.layer.3.output_layer_norm.weight", [768]], ["transformer.layer.3.output_layer_norm.bias", [768]], ["transformer.layer.4.attention.q_lin.weight", [768, 768]], ["transformer.layer.4.attention.q_lin.bias", [768]], ["transformer.layer.4.attention.k_lin.weight", [768, 768]], ["transformer.layer.4.attention.k_lin.bias", [768]], ["transformer.layer.4.attention.v_lin.weight", [768, 768]], ["transformer.layer.4.attention.v_lin.bias", [768]], ["transformer.layer.4.attention.out_lin.weight", [768, 768]], ["transformer.layer.4.attention.out_lin.bias", [768]], ["transformer.layer.4.sa_layer_norm.weight", [768]], ["transformer.layer.4.sa_layer_norm.bias", [768]], ["transformer.layer.4.ffn.lin1.weight", [3072, 768]], ["transformer.layer.4.ffn.lin1.bias", [3072]], ["transformer.layer.4.ffn.lin2.weight", [768, 3072]], ["transformer.layer.4.ffn.lin2.bias", [768]], ["transformer.layer.4.output_layer_norm.weight", [768]], ["transformer.layer.4.output_layer_norm.bias", [768]], ["transformer.layer.5.attention.q_lin.weight", [768, 768]], ["transformer.layer.5.attention.q_lin.bias", [768]], ["transformer.layer.5.attention.k_lin.weight", [768, 768]], ["transformer.layer.5.attention.k_lin.bias", [768]], ["transformer.layer.5.attention.v_lin.weight", [768, 768]], ["transformer.layer.5.attention.v_lin.bias", [768]], ["transformer.layer.5.attention.out_lin.weight", [768, 768]], ["transformer.layer.5.attention.out_lin.bias", [768]], ["transformer.layer.5.sa_layer_norm.weight", [768]], ["transformer.layer.5.sa_layer_norm.bias", [768]], ["transformer.layer.5.ffn.lin1.weight", [3072, 768]], ["transformer.layer.5.ffn.lin1.bias", [3072]], ["transformer.layer.5.ffn.lin2.weight", [768, 3072]], ["transformer.layer.5.ffn.lin2.bias", [768]], ["transformer.layer.5.output_layer_norm.weight", [768]], ["transformer.layer.5.output_layer_norm.bias", [768]]], "output_shape": [[32, 128, 768]], "num_parameters": [23443968, 393216, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768]}, {"name": "classifier.0", "id": 139771767629328, "class_name": "Linear(in_features=768, out_features=250, bias=True)", "parameters": [["weight", [250, 768]], ["bias", [250]]], "output_shape": [[32, 250]], "num_parameters": [192000, 250]}, {"name": "classifier.1", "id": 139771770695248, "class_name": "ReLU()", "parameters": [], "output_shape": [[32, 250]], "num_parameters": []}, {"name": "classifier.2", "id": 139771770691792, "class_name": "Linear(in_features=250, out_features=53, bias=True)", "parameters": [["weight", [53, 250]], ["bias", [53]]], "output_shape": [[32, 53]], "num_parameters": [13250, 53]}], "edges": []}